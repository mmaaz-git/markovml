{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting Started with markovml\n",
    "\n",
    "This tutorial will walk you through using the markovml package for formally verifying properties of Markov processes with learned parameters. We'll start with a quick example and then dive deeper into the features.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, make sure you have the package in your Python path:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add the package to your Python path\n",
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "\n",
    "# Essential imports\n",
    "import numpy as np\n",
    "import gurobipy as gp\n",
    "from markovml.markovml import MarkovReward, MarkovReach, MarkovHitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Start Example\n",
    "\n",
    "Let's start with a simple two-state Markov reward process. Think of this as modeling a system that can be in one of two states (like \"healthy\" and \"sick\"), with learned transition probabilities between them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we set up a `MarkovReward` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restricted license - for non-production use only - expires 2026-11-23\n"
     ]
    }
   ],
   "source": [
    "# Create a simple two-state Markov reward process\n",
    "mrp = MarkovReward(n_states=2, n_features=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the initial state distribution and rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set rewards for each state (1 for state 0, 0 for state 1)\n",
    "mrp.set_r([1, 0])\n",
    "\n",
    "# Set initial state distribution (start in state 0)\n",
    "mrp.set_pi([1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's where things get interesting: we can add a machine learning model and set the parameters of the Markov reward process to be affine functions of the output of the model. Below we train a logistic regression model on some random data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "# Create and train a simple classifier\n",
    "X = np.random.rand(100, 2)  # Random features\n",
    "y = np.random.randint(0, 2, 100)  # Random labels\n",
    "clf = LogisticRegression().fit(X, y)\n",
    "\n",
    "# Add the classifier to the Markov process\n",
    "mrp.add_ml_model(clf)\n",
    "\n",
    "# Link classifier output to transition probabilities\n",
    "# P[0,1] = probability of transitioning from state 0 to 1\n",
    "mrp.set_P([[1 - mrp.ml_outputs[0][0], mrp.ml_outputs[0][0]],\n",
    "           [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe above we first add the ML model to the problem with `add_ml_model`. When we add an ML model, we can access its outputs from `.ml_outputs`. The sole output of the first added model is `ml_outputs[0][0]`.\n",
    "\n",
    "We then set the transition probabilities to be an (affine) function of the ML model's output. Observe that in `set_P`, we can pass a list of expressions or constants. Similarly with `set_r` and `set_pi`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the feature space. By default is is [-inf, inf] for each feature, so we'll constrain them. Note the use of the function `add_feature_constraint`. If you are adding constraints on the feature space, you must use this function. This way, the program knows how to reconstruct the feature space when it solves the smaller optimization problems during the decomposition phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add constraints on the feature space\n",
    "mrp.add_feature_constraint(mrp.features[0] >= 65)\n",
    "mrp.add_feature_constraint(mrp.features[1] >= 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can optimize the total reward. Here we will maximize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'optimal',\n",
       " 'objective': 33.33333333333331,\n",
       " 'values': {'pi': [1.0, 0.0],\n",
       "  'P': [[1.0, 0.0], [0.0, 1.0]],\n",
       "  'r': [1.0, 0.0],\n",
       "  'v': [33.333333333333336, -0.0],\n",
       "  'features': [65.0, 100.0],\n",
       "  'ml_outputs': [[0.0]]}}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Optimize to maximize the total reward\n",
    "mrp.optimize(sense=\"max\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get a dictionary of values that tells us the status of the optimization problem, the objective value, the values of the parameters, features, and ML outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! There are lots of other features and options but this is the basic idea. In the rest of the tutorial, we'll go through each of these steps in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Markov processes\n",
    "\n",
    "There are three objects in the `markovml` package: `MarkovReward`, `MarkovReach`, and `MarkovHitting`. They correspond to the problem you are trying to solve: the first one builds a Markov reward process to optimize the total reward, the second one builds a Markov reachability process to find the probability of reaching a set of states, and the third one builds a Markov hitting process to find the expected hitting time to a set of states. They have slightly different interfaces and internal operations, but there is lots of common functionality, so it is easy to switch between them. (In fact, they all inherit from the same base class, `AbstractMarkov`, which provides almost all of the functionality.)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `MarkovReward`, we need to specify the number of states `n_states` and the number of features `n_features`. Optionally, we can specify the discount factor `discount_factor` (default is 0.97), and optionally pass constants for the parameters which are `pi` (initial state distribution), `P` (transition probabilities), and `r` (rewards).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov = MarkovReward(n_states=2, n_features=2) # must specify these\n",
    "markov = MarkovReward(n_states=2, n_features=2, discount_factor=0.99) # can specify discount\n",
    "markov = MarkovReward(n_states=2, n_features=2, r=[1, 0]) # can specify rewards\n",
    "markov = MarkovReward(n_states=2, n_features=2, pi=[1, 0]) # can specify initial state distribution\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `MarkovReach`, you must specify the number of states `n_states`, the number of features `n_features`, the number of transient states `n_transient`, and the number of target states `n_target`. Optionally, you may pass constants for the parameters which are `pi` (initial distribution over the transient states), `Q` (transitions between transient states), and `R` (transitions from transient to target states).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov = MarkovReach(n_states=2, n_features=2, n_transient=1, n_targets=1) # must specify these\n",
    "markov = MarkovReach(n_states=2, n_features=2, n_transient=1, n_targets=1, pi=[1, 0]) # can specify initial state distribution\n",
    "markov = MarkovReach(n_states=2, n_features=2, n_transient=1, n_targets=1, Q=[[1, 0], [0, 1]])\n",
    "markov = MarkovReach(n_states=2, n_features=2, n_transient=1, n_targets=1, R=[[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For `MarkovHitting`, you must specify the number of states `n_states`, the number of features `n_features`, and the number of transient states `n_transient`. Optionally, you may pass constants for the parameters which are `pi` (initial distribution over the transient states) and `Q` (transitions between transient states). You may wonder why the number of target states is not specified -- this is because it is not needed to formulate the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "markov = MarkovHitting(n_states=2, n_features=2, n_transient=1) # must specify these\n",
    "markov = MarkovHitting(n_states=2, n_features=2, n_transient=1, pi=[1, 0]) # can specify initial state distribution\n",
    "markov = MarkovHitting(n_states=2, n_features=2, n_transient=1, Q=[[1, 0], [0, 1]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, all three classes are very similar. The only difference is in the parameters that define them and their objective functions. The objective function is implemented in the respective classes. Each of these three classes also has a `set_*` function that can be used to set the parameters of the problem, depending on the class; e.g., `set_pi`, `set_P`, `set_Q`, `set_r`, `set_R`. Other than that, every function is the same.\n",
    "\n",
    "In the rest of the tutorial, we will focus on `MarkovReward` since that is the \"richest\" of the three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding ML models\n",
    "\n",
    "The next step is to add a machine learning model to the problem. This is done with the `add_ml_model` function. This function takes a pretrained model as input, creates a MILP formulation of the problem, and adds it to the overall optimization problem. Below we will train a couple of ML models so that we can add them to the problem later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "X = np.random.rand(10000, 10)  # 10 features\n",
    "\n",
    "# regressor for rewards\n",
    "y_reward = 2 * X[:, 0]**2 + np.exp(X[:, 1]) - np.sin(X[:, 2]*3) + np.random.normal(0, 0.1, 10000)\n",
    "reward_model = DecisionTreeRegressor(max_depth=3).fit(X, y_reward)\n",
    "\n",
    "# classifier for transition probabilities\n",
    "y_trans = (X[:, 3] + X[:, 4] > 1).astype(int)\n",
    "trans_model = LogisticRegression().fit(X, y_trans)\n",
    "\n",
    "# regressor for different rewards\n",
    "y_reward2 = -np.log(X[:, 5] + 0.1) + 2 * X[:, 6]**3 + np.random.normal(0, 0.1, 10000)\n",
    "reward_model2 = LinearRegression().fit(X, y_reward2)\n",
    "\n",
    "# classifier for more transition probabilities\n",
    "y_trans2 = (X[:, 7] - 2*X[:, 8] > 0).astype(int)\n",
    "trans_model2 = DecisionTreeClassifier(max_depth=3).fit(X, y_trans2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we initialize a Markov reward process with `5` states and `10` features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp = MarkovReward(n_states=5, n_features=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we add each ML model to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp.add_ml_model(reward_model)\n",
    "mrp.add_ml_model(trans_model)\n",
    "mrp.add_ml_model(reward_model2)\n",
    "mrp.add_ml_model(trans_model2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After adding an ML model, the MILP formulation is added to the overall optimization problem that is being built up in the `MarkovReward` object. We need a way to access the ML outputs, so we store them in the `ml_outputs` attribute. It is a list of dicts, where the keys of the dicts are indices of the outputs: e.g., `ml_outputs[0]` is the dict of outputs of the first ML model added, `ml_outputs[1]` is the dict of outputs of the second ML model added, and so on. If a model outputs a single value, we can access it as `ml_outputs[0][0]`, `ml_outputs[1][0]`, etc. If a model outputs multiple values, we can access them as `ml_outputs[0][0]`, `ml_outputs[0][1]`, `ml_outputs[0][2]`, etc, in the order in which they are outputted by the ML model.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{0: <gurobi.Var ml_outputs_0[0]>},\n",
       " {0: <gurobi.Var ml_outputs_1[0]>},\n",
       " {0: <gurobi.Var ml_outputs_2[0]>},\n",
       " {0: <gurobi.Var ml_outputs_3[0]>}]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp.ml_outputs\n",
    "\n",
    "#type: gurobipy._core.tupledict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Internally, these variables are Gurobi variables, so we can construct expressions using them. In fact the dictionary that stores them is a `tupledict` object, which comes from `gurobipy._core`. This will be useful when we want to link the ML outputs to the parameters of the Markov reward process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We support numerous ML models: see the documentation for a list. Or, you can obtain the list by printing out the following attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(<class 'sklearn.linear_model._base.LinearRegression'>, <class 'sklearn.linear_model._ridge.Ridge'>, <class 'sklearn.linear_model._coordinate_descent.Lasso'>)\n",
      "<class 'sklearn.linear_model._logistic.LogisticRegression'>\n",
      "<class 'sklearn.tree._classes.DecisionTreeRegressor'>\n",
      "<class 'sklearn.tree._classes.DecisionTreeClassifier'>\n",
      "<class 'sklearn.ensemble._forest.RandomForestRegressor'>\n",
      "<class 'sklearn.ensemble._forest.RandomForestClassifier'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPRegressor'>\n",
      "<class 'sklearn.neural_network._multilayer_perceptron.MLPClassifier'>\n",
      "<class 'markovml.utils.models_ext.DecisionRules'>\n",
      "<class 'markovml.utils.models_ext.SequentialClassifier'>\n",
      "<class 'torch.nn.modules.container.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "for key in mrp._ml_model_registry:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are numerous scikit-learn models supported as well as PyTorch `nn.Sequential` models. Some care has to be taken here as to whether the model is a classifier or a regressor. For more details, see the tutorial `tutorials/pytorch_models.ipynb`. We also provide a custom model class called `DecisionRules` that can be used to construct \"if-then\" rules in a natural language-like syntax. For more details on this, see the tutorial `tutorials/decisionrules.ipynb`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We allow the parameters of the Markov process to be bound via *affine equalities* to the ML outputs. Of course, this also means they can be set to constants. There are three ways to do this. \n",
    "1. Pass constants at initialization.\n",
    "2. Use the `set_to_const` or `set_to_ml_output` functions.\n",
    "3. Use the `set_*` helper functions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If passing values at initialization, they must be constants. Why? Because at initialization, the ML models have not yet been added, so we cannot refer to the ML output variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<markovml.markovml.MarkovReward at 0x176753050>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pass constants at initialization\n",
    "MarkovReward(n_states=2, n_features=2, r=[1, 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `set_to_const` and `set_to_ml_output` functions can be used to set the parameters one at a time. They take a variable and a value as input. If we call `set_to_const`, the value must be a constant. If we call `set_to_ml_output`, the value must be an expression that uses the ML output variables. Let's try this out with our `mrp` object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setting to constants\n",
    "mrp.set_to_const(mrp.r[0], 1)\n",
    "mrp.set_to_const(mrp.r[1], 0)\n",
    "\n",
    "# setting to functions of ML outputs\n",
    "mrp.set_to_ml_output(mrp.r[2], mrp.ml_outputs[0][0])\n",
    "mrp.set_to_ml_output(mrp.r[3], 2*mrp.ml_outputs[0][0]-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the first ML model we added was a regressor for rewards, so we obtain its output and set it equal to `mrp.r[2]`, and we make `mrp.r[3]` equal to `2*mrp.ml_outputs[0][0]-1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the parameters of the Markov reward process are `pi`, `P`, and `r`, and we can call them with specific indices to get the variable. Again, internally, these are Gurobi variables, which enables us to construct expressions using them. See section on \"Building Markov processes\" for the analogs in the case of `MarkovReach` and `MarkovHitting`.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An important caveat to note is that once a parameter has been set, it cannot be changed, due to how things are constructed internally. We will get the following error now if we try to set `mrp.r[2]` to `1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "r[2] is already set to a constant or ML output",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# expecting ValueError\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mmrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_to_const\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mr\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/markovml/tutorials/../markovml/markovml.py:572\u001b[0m, in \u001b[0;36mAbstractMarkov.set_to_const\u001b[0;34m(self, var, value)\u001b[0m\n\u001b[1;32m    570\u001b[0m     i \u001b[38;5;241m=\u001b[39m indices\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_is_const[base_name][i] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_is_ml_output[base_name][i]:\n\u001b[0;32m--> 572\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mvar_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is already set to a constant or ML output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    573\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_is_const[base_name][i] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    574\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m:\n",
      "\u001b[0;31mValueError\u001b[0m: r[2] is already set to a constant or ML output"
     ]
    }
   ],
   "source": [
    "# expecting ValueError\n",
    "mrp.set_to_const(mrp.r[2], 1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, it may get cumbersome to set the parameters one at a time. This is where the `set_*` helper functions come in, which allows us to set all of `pi`, `P`, or `r` at once. We will use it to set the `P` and `r` parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp.set_pi([mrp.ml_outputs[1][0],                # pi[0] = p\n",
    "            1 - mrp.ml_outputs[1][0],            # pi[1] = 1-p\n",
    "            0, 0, 0])\n",
    "\n",
    "# Complex transition probabilities\n",
    "# Set P using mix of ML outputs and constants\n",
    "# Each row must sum to 1\n",
    "mrp.set_P([\n",
    "    # Row 0: ML-based transitions\n",
    "    [mrp.ml_outputs[1][0],                    # p\n",
    "     1 - mrp.ml_outputs[1][0] - 0.2,         # 1-p-0.2\n",
    "     0.1,                                     # fixed\n",
    "     0.1,                                     # fixed\n",
    "     0],                                      # fixed\n",
    "\n",
    "    # Row 1: Different ML model\n",
    "    [mrp.ml_outputs[3][0],                    # q\n",
    "     0.3,                                     # fixed\n",
    "     0.7 - mrp.ml_outputs[3][0],             # 0.7-q\n",
    "     0,                                       # fixed\n",
    "     0],                                      # fixed\n",
    "\n",
    "    # Row 2: Combination of both ML models\n",
    "    [0,                                       # fixed\n",
    "     mrp.ml_outputs[1][0],                   # p\n",
    "     mrp.ml_outputs[3][0],                   # q\n",
    "     1 - mrp.ml_outputs[1][0] - mrp.ml_outputs[3][0],  # 1-p-q\n",
    "     0],                                      # fixed\n",
    "\n",
    "    # Row 3: More ML combinations\n",
    "    [0,                                       # fixed\n",
    "     0.2,                                     # fixed\n",
    "     mrp.ml_outputs[1][0],                   # p\n",
    "     0.5,                                     # fixed\n",
    "     0.3 - mrp.ml_outputs[1][0]],            # 0.3-p\n",
    "\n",
    "    # Row 4: Absorbing state\n",
    "    [0, 0, 0, 0, 1]                          # fixed\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding linear inequalities on the parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can add linear inequalities on the parameters of the Markov process. This is done with the `add_constraint` function. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp.add_constraint(mrp.r[0] >= 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, you can add arbitrarily complex linear inequalities on the parameters. For example, you can place ordering constraints on the parameters, like `mrp.add_constraint(mrp.r[0] >= mrp.r[1])`. A common such constraint is *increasing failure rate*, which enforces a stochastic ordering on the probabilities. There is a helper function for this: `mrp.add_ifr_inequalities()`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next crucial step is to define the feature space, referred to as $\\mathcal{X}$ in the paper. Our only assumption is that the feature space is MILP-representable. In order to do this, there are two basic functions: `add_feature_constraint` and `add_feature_aux_variable`. The first function is to add linear inequalities, while the second allows to add additional auxiliary variables to the Gurobi model, which can be binary, integer, or continuous, and with prespecified bounds. These two building blocks allow us to define any arbitrary MILP-representable feature space.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's an example of adding some linear inequalities on the feature space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp.add_feature_constraint(mrp.features[0] + mrp.features[1] <= 1.5)\n",
    "mrp.add_feature_constraint(mrp.features[2] >= mrp.features[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can introduce a binary auxiliary variable to the problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gurobipy import GRB\n",
    "aux = mrp.add_feature_aux_variable(name=\"aux\", vtype=GRB.BINARY)\n",
    "mrp.add_feature_constraint(mrp.features[4] + mrp.features[5] <= 1 + aux)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When adding an auxiliary variable, all keyword arguments are passed to the `addVar` function of Gurobi, meaning you can specify bounds, types, names, etc. See the Gurobi documentation for more details."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Also, recall that every modeling variable in our class is a Gurobi variable, so we can also modify their properties as in any Gurobi model. If you are familiar with Gurobi, this will be very natural. For example, below we set the bounds of each feature to be [-1, 1].\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature space constraints\n",
    "for i in range(mrp.n_features):\n",
    "    mrp.features[i].LB = -1\n",
    "    mrp.features[i].UB = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using `add_feature_constraint` and `add_feature_aux_variable` is important because then the program knows how to reconstruct the feature space when solving the smaller MILPs for each ML model. You can always add any constraints you wish by directly accessing `.model`, which is the Gurobi model object. But, by using these helper functions, you can ensure that the decomposition step works properly."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizing!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can optimize the problem!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "r[4] needs to be set to a constant or linked to an ML output",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mmrp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43msense\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmax\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Research/markovml/tutorials/../markovml/markovml.py:767\u001b[0m, in \u001b[0;36mAbstractMarkov.optimize\u001b[0;34m(self, use_decomp, sense, verbose, gurobi_params)\u001b[0m\n\u001b[1;32m    721\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    722\u001b[0m \u001b[38;5;124;03mPublic interface for optimizing the problem.\u001b[39;00m\n\u001b[1;32m    723\u001b[0m \u001b[38;5;124;03mIf use_decomp is True, we use the decomposition and bound propagation approach.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03mand bound propagation approach, and lastly solves the bilinear program.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    766\u001b[0m \u001b[38;5;66;03m# Check if all required variables are set\u001b[39;00m\n\u001b[0;32m--> 767\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_all_vars_set\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_decomp:\n\u001b[1;32m    770\u001b[0m     \u001b[38;5;66;03m# Step 1: ML bounds\u001b[39;00m\n\u001b[1;32m    771\u001b[0m     ml_bounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_ml_bounds(verbose\u001b[38;5;241m=\u001b[39mverbose)\n",
      "File \u001b[0;32m~/Documents/Research/markovml/tutorials/../markovml/markovml.py:1514\u001b[0m, in \u001b[0;36mMarkovReward._check_all_vars_set\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1512\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_dim[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m   1513\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_is_ml_output[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m][i] \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_is_const[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m][i]):\n\u001b[0;32m-> 1514\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr[\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] needs to be set to a constant or linked to an ML output\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_dim[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m0\u001b[39m]):\n\u001b[1;32m   1516\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_var_dim[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m1\u001b[39m]):\n",
      "\u001b[0;31mValueError\u001b[0m: r[4] needs to be set to a constant or linked to an ML output"
     ]
    }
   ],
   "source": [
    "mrp.optimize(sense=\"max\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Aha! Not so fast! Before optimizing, all sorts of checks happen. Namely, all the parameters `pi`, `P`, and `r` *must* be bound via an affine equality (or to a constant). Here, the program indicates that `r[4]` has not been bound to anything.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mrp.set_to_const(mrp.r[4], 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try optimizing again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 4 rows, 20 columns and 15 nonzeros\n",
      "Model fingerprint: 0xd2bf7c56\n",
      "Model has 28 simple general constraints\n",
      "  28 INDICATOR\n",
      "Variable types: 11 continuous, 9 integer (9 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "  GenCon rhs range [3e-01, 3e+00]\n",
      "  GenCon coe range [1e+00, 1e+00]\n",
      "Variable types: 39 continuous, 9 integer (9 binary)\n",
      "Found heuristic solution: objective 3.2503723\n",
      "\n",
      "Root relaxation: unbounded, 0 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     2  postponed    0         3.25037          -      -     -    0s\n",
      "*    2     2               1       0.7791024          -      -   5.0    0s\n",
      "\n",
      "Explored 15 nodes (30 simplex iterations) in 0.01 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 0.779102 3.25037 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 7.791024108888e-01, best bound 7.791024108888e-01, gap 0.0000%\n",
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 4 rows, 20 columns and 15 nonzeros\n",
      "Model fingerprint: 0x51d530dd\n",
      "Model has 28 simple general constraints\n",
      "  28 INDICATOR\n",
      "Variable types: 11 continuous, 9 integer (9 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "  GenCon rhs range [3e-01, 3e+00]\n",
      "  GenCon coe range [1e+00, 1e+00]\n",
      "Variable types: 39 continuous, 9 integer (9 binary)\n",
      "Found heuristic solution: objective 3.2503723\n",
      "\n",
      "Root relaxation: unbounded, 0 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     2  postponed    0         3.25037          -      -     -    0s\n",
      "\n",
      "Explored 15 nodes (30 simplex iterations) in 0.01 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 3.25037 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 3.250372329036e+00, best bound 3.250372329036e+00, gap 0.0000%\n",
      "ML model 0 output 0 bounds: 0.7791024108887881, 3.2503723290363933\n",
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 5 rows, 14 columns and 20 nonzeros\n",
      "Model fingerprint: 0xf1e02c5f\n",
      "Model has 2 simple general constraints\n",
      "  2 INDICATOR\n",
      "Variable types: 12 continuous, 2 integer (2 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-02, 2e+01]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+01]\n",
      "  GenCon coe range [1e+00, 1e+00]\n",
      "Variable types: 13 continuous, 4 integer (4 binary)\n",
      "Found heuristic solution: objective 0.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 0 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 0.000000000000e+00, best bound 0.000000000000e+00, gap 0.0000%\n",
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 5 rows, 14 columns and 20 nonzeros\n",
      "Model fingerprint: 0xf23fc244\n",
      "Model has 2 simple general constraints\n",
      "  2 INDICATOR\n",
      "Variable types: 12 continuous, 2 integer (2 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e-02, 2e+01]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+01]\n",
      "  GenCon coe range [1e+00, 1e+00]\n",
      "Variable types: 13 continuous, 4 integer (4 binary)\n",
      "Found heuristic solution: objective -0.0000000\n",
      "Found heuristic solution: objective 1.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 2: 1 -0 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.000000000000e+00, best bound 1.000000000000e+00, gap 0.0000%\n",
      "ML model 1 output 0 bounds: -0.0, 1.0\n",
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 4 rows, 12 columns and 18 nonzeros\n",
      "Model fingerprint: 0xb37f0366\n",
      "Variable types: 11 continuous, 1 integer (1 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [8e-04, 2e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Variable types: 11 continuous, 1 integer (1 binary)\n",
      "Found heuristic solution: objective -2.5882056\n",
      "\n",
      "Root relaxation: interrupted, 0 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "Explored 1 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: -2.58821 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective -2.588205569817e+00, best bound -2.588264417126e+00, gap 0.0023%\n",
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 4 rows, 12 columns and 18 nonzeros\n",
      "Model fingerprint: 0xca1526ff\n",
      "Variable types: 11 continuous, 1 integer (1 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [8e-04, 2e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "Variable types: 11 continuous, 1 integer (1 binary)\n",
      "Found heuristic solution: objective 5.1871774\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 5.18718 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 5.187177370916e+00, best bound 5.187177370916e+00, gap 0.0000%\n",
      "ML model 2 output 0 bounds: -2.5882055698174034, 5.1871773709163564\n",
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 4 rows, 20 columns and 15 nonzeros\n",
      "Model fingerprint: 0x3ff89386\n",
      "Model has 27 simple general constraints\n",
      "  27 INDICATOR\n",
      "Variable types: 11 continuous, 9 integer (9 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "  GenCon rhs range [1e-02, 1e+00]\n",
      "  GenCon coe range [1e+00, 1e+00]\n",
      "Variable types: 38 continuous, 9 integer (9 binary)\n",
      "Found heuristic solution: objective 1.0000000\n",
      "\n",
      "Root relaxation: objective 0.000000e+00, 10 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0    0.00000    0    1    1.00000    0.00000   100%     -    0s\n",
      "H    0     0                       0.3062500    0.00000   100%     -    0s\n",
      "H    0     0                       0.0100937    0.00000   100%     -    0s\n",
      "     0     0    0.00000    0    1    0.01009    0.00000   100%     -    0s\n",
      "\n",
      "Explored 1 nodes (10 simplex iterations) in 0.01 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 0.0100937 0.30625 1 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.009372746936e-02, best bound 1.009372746936e-02, gap 0.0000%\n",
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 4 rows, 20 columns and 15 nonzeros\n",
      "Model fingerprint: 0x6b940ae2\n",
      "Model has 27 simple general constraints\n",
      "  27 INDICATOR\n",
      "Variable types: 11 continuous, 9 integer (9 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [1e+00, 1e+00]\n",
      "  Objective range  [1e+00, 1e+00]\n",
      "  Bounds range     [1e+00, 1e+00]\n",
      "  RHS range        [1e+00, 2e+00]\n",
      "  GenCon rhs range [1e-02, 1e+00]\n",
      "  GenCon coe range [1e+00, 1e+00]\n",
      "Variable types: 38 continuous, 9 integer (9 binary)\n",
      "Found heuristic solution: objective 1.0000000\n",
      "\n",
      "Explored 0 nodes (0 simplex iterations) in 0.00 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 1: 1 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 1.000000000000e+00, best bound 1.000000000000e+00, gap 0.0000%\n",
      "ML model 3 output 0 bounds: 0.010093727469360329, 1.0\n",
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 50 rows, 73 columns and 127 nonzeros\n",
      "Model fingerprint: 0x12733c4a\n",
      "Model has 5 quadratic objective terms\n",
      "Model has 5 quadratic constraints\n",
      "Model has 57 simple general constraints\n",
      "  57 INDICATOR\n",
      "Variable types: 55 continuous, 18 integer (18 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [8e-04, 2e+01]\n",
      "  QMatrix range    [1e+00, 1e+00]\n",
      "  QLMatrix range   [1e+00, 1e+00]\n",
      "  Objective range  [0e+00, 0e+00]\n",
      "  QObjective range [2e+00, 2e+00]\n",
      "  Bounds range     [1e-02, 3e+02]\n",
      "  RHS range        [1e-01, 2e+01]\n",
      "  GenCon rhs range [1e-02, 3e+00]\n",
      "  GenCon coe range [1e+00, 1e+00]\n",
      "\n",
      "Solving non-convex MIQCP\n",
      "\n",
      "Variable types: 101 continuous, 42 integer (19 binary)\n",
      "Found heuristic solution: objective 280.5548781\n",
      "\n",
      "Root relaxation: objective 3.181861e+02, 50 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  318.18613    0    6  280.55488  318.18613  13.4%     -    0s\n",
      "     0     0  318.18613    0    7  280.55488  318.18613  13.4%     -    0s\n",
      "     0     0  318.15980    0    9  280.55488  318.15980  13.4%     -    0s\n",
      "     0     0  318.15980    0    9  280.55488  318.15980  13.4%     -    0s\n",
      "     0     0  288.74789    0    2  280.55488  288.74789  2.92%     -    0s\n",
      "H    0     0                     284.3445415  288.74789  1.55%     -    0s\n",
      "     0     0  288.74789    0    2  284.34454  288.74789  1.55%     -    0s\n",
      "H    0     2                     288.7478892  288.74789  0.00%     -    0s\n",
      "     0     2  288.74789    0    2  288.74789  288.74789  0.00%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 3\n",
      "  RLT: 2\n",
      "\n",
      "Explored 1 nodes (58 simplex iterations) in 0.02 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 288.748 284.345 280.555 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.887478892425e+02, best bound 2.887478892425e+02, gap 0.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'optimal',\n",
       " 'objective': 288.747889242459,\n",
       " 'values': {'pi': [-0.0, 1.0, -0.0, -0.0, -0.0],\n",
       "  'P': [[-0.0, 0.8, 0.1, 0.1, 0.0],\n",
       "   [0.010093727469360329, 0.3, 0.6899062725306396, -0.0, -0.0],\n",
       "   [-0.0, 0.0, 0.010093727469360329, 0.9899062725306397, -0.0],\n",
       "   [-0.0, 0.2, -0.0, 0.5, 0.3],\n",
       "   [-0.0, -0.0, -0.0, -0.0, 1.0]],\n",
       "  'r': [1.0, 0.0, 3.2503723290363933, 5.500744658072787, 10.0],\n",
       "  'v': [284.1957210570221,\n",
       "   288.747889242459,\n",
       "   301.7587804206267,\n",
       "   307.80162169147525,\n",
       "   333.3333333333333],\n",
       "  'features': [0.8362411260604847,\n",
       "   0.5964297652244557,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   0.3880178779363621,\n",
       "   0.12625254690647303,\n",
       "   1.0],\n",
       "  'ml_outputs': [[3.2503723290363933],\n",
       "   [0.0],\n",
       "   [1.502729593951862],\n",
       "   [0.010093727469360329]]}}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp.optimize(sense=\"max\", verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem solves to optimality, and returns a dictionary of the status, objective value, and values of the decision variables. You can control various settings of the optimization with the arguments to the `optimize` function. For example, the sense can be either `min` or `max`, and the `verbose` argument controls whether the Gurobi log is printed. You can also pass additional paramters to the Gurobi solver using the `gurobi_params` argument, which accepts a dictionary. With this you can control things like the time limit, the optimality gap, and so on. See the Gurobi documentation for more details. \n",
    "\n",
    "There is also an argument `use_decomp` which controls whether the decomposition and bound propagation scheme from the paper is used. You can disable it if you wish for benchmarking purposes. Otherwise, there is practically no reason to disable it. We can try optimizing without decomposition now. One thing to note is that because the Gurobi model persists, we need to reset it before optimizing again in order to have a fair comparison of the runtime (i.e., the Gurobi model still stores the previous optimal solution)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded solution information\n",
      "Set parameter OutputFlag to value 1\n",
      "Set parameter Presolve to value 0\n",
      "Gurobi Optimizer version 12.0.0 build v12.0.0rc1 (mac64[arm] - Darwin 23.3.0 23D2057)\n",
      "\n",
      "CPU model: Apple M3\n",
      "Thread count: 8 physical cores, 8 logical processors, using up to 8 threads\n",
      "\n",
      "Non-default parameters:\n",
      "Presolve  0\n",
      "\n",
      "Optimize a model with 50 rows, 73 columns and 127 nonzeros\n",
      "Model fingerprint: 0x12733c4a\n",
      "Model has 5 quadratic objective terms\n",
      "Model has 5 quadratic constraints\n",
      "Model has 57 simple general constraints\n",
      "  57 INDICATOR\n",
      "Variable types: 55 continuous, 18 integer (18 binary)\n",
      "Coefficient statistics:\n",
      "  Matrix range     [8e-04, 2e+01]\n",
      "  QMatrix range    [1e+00, 1e+00]\n",
      "  QLMatrix range   [1e+00, 1e+00]\n",
      "  Objective range  [0e+00, 0e+00]\n",
      "  QObjective range [2e+00, 2e+00]\n",
      "  Bounds range     [1e-02, 3e+02]\n",
      "  RHS range        [1e-01, 2e+01]\n",
      "  GenCon rhs range [1e-02, 3e+00]\n",
      "  GenCon coe range [1e+00, 1e+00]\n",
      "\n",
      "Solving non-convex MIQCP\n",
      "\n",
      "Variable types: 101 continuous, 42 integer (19 binary)\n",
      "Found heuristic solution: objective 280.5548781\n",
      "\n",
      "Root relaxation: objective 3.181861e+02, 50 iterations, 0.00 seconds (0.00 work units)\n",
      "\n",
      "    Nodes    |    Current Node    |     Objective Bounds      |     Work\n",
      " Expl Unexpl |  Obj  Depth IntInf | Incumbent    BestBd   Gap | It/Node Time\n",
      "\n",
      "     0     0  318.18613    0    6  280.55488  318.18613  13.4%     -    0s\n",
      "     0     0  318.18613    0    7  280.55488  318.18613  13.4%     -    0s\n",
      "     0     0  318.15980    0    9  280.55488  318.15980  13.4%     -    0s\n",
      "     0     0  318.15980    0    9  280.55488  318.15980  13.4%     -    0s\n",
      "     0     0  288.74789    0    2  280.55488  288.74789  2.92%     -    0s\n",
      "H    0     0                     284.3445415  288.74789  1.55%     -    0s\n",
      "     0     0  288.74789    0    2  284.34454  288.74789  1.55%     -    0s\n",
      "H    0     2                     288.7478892  288.74789  0.00%     -    0s\n",
      "     0     2  288.74789    0    2  288.74789  288.74789  0.00%     -    0s\n",
      "\n",
      "Cutting planes:\n",
      "  Implied bound: 3\n",
      "  RLT: 2\n",
      "\n",
      "Explored 1 nodes (58 simplex iterations) in 0.03 seconds (0.00 work units)\n",
      "Thread count was 8 (of 8 available processors)\n",
      "\n",
      "Solution count 3: 288.748 284.345 280.555 \n",
      "\n",
      "Optimal solution found (tolerance 1.00e-04)\n",
      "Best objective 2.887478892425e+02, best bound 2.887478892425e+02, gap 0.0000%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'optimal',\n",
       " 'objective': 288.747889242459,\n",
       " 'values': {'pi': [-0.0, 1.0, -0.0, -0.0, -0.0],\n",
       "  'P': [[-0.0, 0.8, 0.1, 0.1, 0.0],\n",
       "   [0.010093727469360329, 0.3, 0.6899062725306396, -0.0, -0.0],\n",
       "   [-0.0, 0.0, 0.010093727469360329, 0.9899062725306397, -0.0],\n",
       "   [-0.0, 0.2, -0.0, 0.5, 0.3],\n",
       "   [-0.0, -0.0, -0.0, -0.0, 1.0]],\n",
       "  'r': [1.0, 0.0, 3.2503723290363933, 5.500744658072787, 10.0],\n",
       "  'v': [284.1957210570221,\n",
       "   288.747889242459,\n",
       "   301.7587804206267,\n",
       "   307.80162169147525,\n",
       "   333.3333333333333],\n",
       "  'features': [0.8362411260604847,\n",
       "   0.5964297652244557,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   0.3880178779363621,\n",
       "   0.12625254690647303,\n",
       "   1.0],\n",
       "  'ml_outputs': [[3.2503723290363933],\n",
       "   [0.0],\n",
       "   [1.502729593951862],\n",
       "   [0.010093727469360329]]}}"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp.model.reset()\n",
    "mrp.optimize(sense=\"max\", verbose=True, use_decomp=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get the same objective value!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of optimizing, we can also find a feasible solution, as below. You can pass a lower bound and an upper bound for the objective value, and it will determine if a feasible solution exists within that range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discarded solution information\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'status': 'feasible',\n",
       " 'values': {'pi': [0.0, 1.0, 0.0, 0.0, 0.0],\n",
       "  'P': [[0.0, 0.8, 0.1, 0.1, 0.0],\n",
       "   [0.16236162361623419, 0.3, 0.5376383763837648, 0.0, 0.0],\n",
       "   [0.0, 0.0, 0.1623616236162343, 0.8376383763837649, 0.0],\n",
       "   [0.0, 0.2, 0.0, 0.5, 0.3],\n",
       "   [0.0, 0.0, 0.0, 0.0, 1.0]],\n",
       "  'r': [1.0, 0.0, 3.2503723290363933, 5.500744658072787, 10.0],\n",
       "  'v': [277.14710364141985,\n",
       "   280.554878054622,\n",
       "   297.7230103596665,\n",
       "   304.71532233139675,\n",
       "   333.3333333333333],\n",
       "  'features': [0.8362411260604847,\n",
       "   0.5964297652244557,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   -1.0,\n",
       "   1.0,\n",
       "   0.43699599802494227,\n",
       "   1.0],\n",
       "  'ml_outputs': [[3.2503723290363933],\n",
       "   [0.0],\n",
       "   [1.4932539761713508],\n",
       "   [0.16236162361623552]]}}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp.model.reset()\n",
    "mrp.find_feasible(lb=0, ub=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And again, this time with a tighter bound."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'status': 'infeasible', 'message': 'Problem is infeasible'}"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mrp.model.reset()\n",
    "mrp.find_feasible(lb=0, ub=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we get that the problem is infeasible! By the way, if you wish to keep one of the ends \"open\", i.e., $\\pm \\infty$, you can pass `None` for the bound. But you have to specify at least one bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And that's it! You now understand the full workflow of the `markovml` package. You are ready to start using it. For more details, you can browse the documentation or the other tutorials."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
